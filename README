Справка:
usage: main.py [-h] [-c CONFIG_NAME] [-u URL]

Get html by url and extract text data.

optional arguments:
  -h, --help            show this help message and exit
  -c CONFIG_NAME, --config CONFIG_NAME
                        config file name (default: "default.json")
  -u URL, --url URL     url (default: "https://lenta.ru/articles/2020/12/02/ad/")

Программа принимает в ключе -u урл, скачивает его, извлекает текст и сохраняет его в файл.

Особенности реализации:
1) Все действия - скачивание документа, сохранение файла - синхронные.
  Для консольной утилиты, работающей с одним урлом, этого кажется достаточно.
2) Особенности обработки документа хранятся в json файле. Имя этого файла программа получает по ключу -c.
3) Архитектура: каждая часть системы - чтение настроек, краулер, экстрактор текста,
  форматтер текста, сохранялка - сделаны в отдельных классах.
  Т.к. может понадобиться заменить json для хранения настроек на что-нибудь другое, сделать асинхронные краулер и
  сохранялку, изменить правила извлечения текста, изменить форматирование - все это можно легко сделать, независимо
  для различных частей системы.
4) Конфиг:
  - я выбрал json для хранения (вместо ini, bson или protobuf), т.к. его удобно править человеку, с ним
    легко работать, удобно хранить секции для разных модулей, легко добавить или изменить элементы.
  - хранятся: ширина экрана, для каждого типа блока - количество пустых строчек следующих за блоком,
    отступы - количество и символ (их я не успел реализовать в коде).
  - сохранять ли оригинальный документ
5) main.py - точка входа
  парсит аргументы командной строки, создает все необходимые объекты частей системы,
  вызывает их в нужной последовательности.
6) crauler.py - скачивание документа
  использует requests
  сделано самым простым методом, работает синхронно. Возвращает тело документа.
  объект callable, т.е. можно звать просто crauler(url) вместо crauler.get_data(url)
7) extractor.py - извлечение текста
  использует lxml
  ищет в документе блоки h1, h2, p. Извлекает из них текст.
  Ссылки не успел сделать.
8) formatter.py - форматирование текста
  делит текст на строчки нужной ширины. Добавляет пустые строки после блоков, количество задается в конфиге.
9) saver.py - сохранение текста
  сохраняет получившийся текст. Если в конфиге указано - то сохраняет и исходный документ.
  работает синхронно.
  преобразует урл в путь в файловой системе, создает нужные каталоги.
10) setup.py - установка пакета
  здесь прописаны данные пакета - имя, версия, зависимости
11) test_formatter.py
  использует pytest
  здесь тестируется функция из модуля форматирования.
12) data/*
  пример результата работы - сохраненный оригинальный документ и извлеченный из-него текст
  тот же текстовый файл лежит по правильному пути в lenta.ru

Улучшения:
0) Улучшить экранирование - и в извлечении текста, и в построении пути файла.
1) Использовать список урлов
  в этом случае стоит подумать об асинхронном краулере, асинхронной сохранялке.
  Нужно делать проверку что сохранялка не будет затирать данные.
2) Проходить рекурсивно по ссылкам в тексте/документе
  вероятно в этом случае захочется сохранять структуру ссылок. Потребуется маппинг ссылок.
  Может быть, захочется хранить такую структуру документов в графовой БД.
3) Использовать прокси для скачивания документов.
4) Сейчас сохраняется текст только уже присутствующий в html документе. Все, что должно добавляться скриптами -
  сохранено не будет. Чтобы справиться с этой проблемой нужно выполнять скрипты, т.е. по сути, сделать браузер.
5) Преобразование утилиты в сервис.
  Утилита работает на сервере, ей поступают запросы с урлами, она где-нибудь складывает извлеченный текст
  и выдает доступ к сохраненному тексту (это могут быть разные сервисы).
  Здесь захочется добавить очередь перед краулером, очередь перед сохранялкой. Сохранять в базу данных.
  В зависимости от нагрузки добавить репликацию для утилиты, репликацию и шардинг для базы данных,
  балансировку перед сервисом и перед БД.